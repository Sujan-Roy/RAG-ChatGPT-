「An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale」というタイトルの論文が、International Conference on Learning Representations(ICLR)2021でGoogle Researchのチームによって発表されました。この論文では、自然言語処理(NLP)で一般的に使用されるTransformerアーキテクチャのコンピュータービジョンタスクへの適用を探ります。著者らは、画像をパッチのシーケンスに再形成し、画像分類用の標準のTransformerエンコーダーで処理するVision Transformer(ViT)モデルを紹介しています。

ViTモデルは、大規模なデータセットで事前トレーニングした場合、ImageNet、CIFAR-100、VTABなどのさまざまな画像認識ベンチマークで強力なパフォーマンスを発揮します。最先端の畳み込みネットワークと比較して競争力のある結果を達成し、トレーニングに必要な計算リソースが少なくて済みます。この論文では、セルフアテンションメカニズムを畳み込みアーキテクチャに統合する際の課題について議論し、従来の畳み込みネットワークと比較したViTの効率性とスケーラビリティに焦点を当てています。

この研究では、ViTモデルをNLPタスクで使用されていた以前のTransformerベースのモデルと比較し、画像処理にTransformerを適用するためのさまざまなアプローチを探ります。また、事前学習データサイズがViT性能に与える影響についても議論し、計算リソースを最小限に抑えながら高精度を実現するための大規模学習の重要性を強調しています。さらに、ビジョントランスフォーマーの内部動作を掘り下げ、画像データの処理方法と、モデルのパフォーマンスに対する位置埋め込みの重要性に焦点を当てています。

さらに、この論文では、サイズが大きくなるデータセットで事前学習したViTモデルの転送性能に関する実験を提示し、より大きなデータセットで事前学習した場合のViTモデルがResNetよりも優れた性能を発揮することを示しています。また、この研究では、さまざまなデータセットでViTモデルの効率とパフォーマンスのトレードオフをResNetsと比較して評価し、モデルのパフォーマンスを向上させる手法としてAxial Attentionを導入しています。

結論として、この論文では、ViTモデル、さまざまなデータセットでのそのパフォーマンス、位置埋め込みの影響、および軸注意ベースのモデルの効率の包括的な分析を提供します。これは、コンピュータービジョンタスクにおけるTransformersの探索に貢献し、画像認識アプリケーション向けのViTモデルの可能性を強調しています。この論文はICLR 2021で発表され、画像認識モデルの進歩とその計算効率とスケーラビリティを紹介しました。
